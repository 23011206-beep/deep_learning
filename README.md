# Deep Learning - PCB Classification

Dá»± Ã¡n phÃ¢n loáº¡i PCB (Printed Circuit Board) sá»­ dá»¥ng Multi-Layer Perceptron (MLP) Ä‘Æ°á»£c xÃ¢y dá»±ng hoÃ n toÃ n báº±ng NumPy.

## ğŸ“‹ Tá»•ng quan

ChÆ°Æ¡ng trÃ¬nh nÃ y huáº¥n luyá»‡n má»™t máº¡ng neural network Ä‘á»ƒ phÃ¢n loáº¡i áº£nh PCB thÃ nh 2 lá»›p:
- **PCB**: áº¢nh chá»©a máº¡ch in
- **Not PCB**: áº¢nh khÃ´ng chá»©a máº¡ch in

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Cáº¥u trÃºc module (tá»« trá»«u tÆ°á»£ng Ä‘áº¿n cá»¥ thá»ƒ)

```
+------------------------------------------------------------+
|                        train.py                            |
|                  (Orchestration Layer)                     |
+------------------------------------------------------------+
| - Äiá»u phá»‘i toÃ n bá»™ quÃ¡ trÃ¬nh training                     |
| - Quáº£n lÃ½ vÃ²ng láº·p training/validation                     |
| - Early stopping & model checkpoint                        |
| - Logging & evaluation                                     |
+------------------------------------------------------------+
                |                          |
                v                          v
+----------------------------+  +----------------------------+
|         model.py           |  |       data_utils.py        |
|       (Core Logic)         |  |       (Data Layer)         |
+----------------------------+  +----------------------------+
| - Activation functions     |  | - Data augmentation        |
| - Forward propagation      |  | - Äá»c vÃ  load dataset      |
| - Backward propagation     |  | - Normalization            |
| - Loss calculation         |  | - Batching                 |
+----------------------------+  +----------------------------+
                |                          |
                v                          v
+------------------------------------------------------------+
|                      inference.py                          |
|                   (Application Layer)                      |
+------------------------------------------------------------+
| - Sá»­ dá»¥ng model Ä‘Ã£ train Ä‘á»ƒ dá»± Ä‘oÃ¡n                        |
| - Visualize káº¿t quáº£                                        |
+------------------------------------------------------------+
```

---

## ğŸ“ Chi tiáº¿t cÃ¡c file chÃ­nh

### 1. **`model.py`** - Core Neural Network Logic
Äá»‹nh nghÄ©a toÃ¡n há»c vÃ  logic cá»§a neural network

#### Chá»©c nÄƒng chÃ­nh:

##### ğŸ”¹ Activation Functions (HÃ m kÃ­ch hoáº¡t)
```python
sigmoid(z)           # Sigmoid: 1/(1+e^-z)
relu(z)              # ReLU: max(0, z)
softmax(z)           # Softmax: e^zi / Î£e^zj
dropout(A, keep_prob) # Dropout regularization
```

##### ğŸ”¹ Model Initialization
```python
initialize_mlp_classification_model(layer_dims)
# Khá»Ÿi táº¡o weights vÃ  biases cho táº¥t cáº£ layers
# Sá»­ dá»¥ng He initialization cho ReLU
```

##### ğŸ”¹ Forward Propagation
```python
forward_propagation(model, X, keep_prob, training)
# TÃ­nh toÃ¡n output tá»« input qua táº¥t cáº£ layers
# Ãp dá»¥ng dropout náº¿u Ä‘ang training
# Cache cÃ¡c giÃ¡ trá»‹ trung gian cho backward pass
```

##### ğŸ”¹ Loss Function
```python
cross_entropy_loss(output, label, model, lambda_reg)
# Cross-entropy loss cho classification
# TÃ¹y chá»n thÃªm L2 regularization
```

##### ğŸ”¹ Backward Propagation
```python
backward_propagation(model, cache, X, label, keep_prob)
# TÃ­nh gradient cá»§a loss theo tá»«ng parameter
# Backpropagate qua táº¥t cáº£ layers

calculate_gradient(model, cache, X, label, lambda_reg, keep_prob)
# Wrapper cho backward propagation
# ThÃªm L2 regularization gradient náº¿u cáº§n

update_parameters(model, gradients, learning_rate)
# Cáº­p nháº­t weights vÃ  biases theo gradient descent
```

**Input/Output**:
- Input: Raw data (numpy arrays), model parameters
- Output: Predictions, gradients, updated parameters

---

### 2. **`data_utils.py`** - Data Processing Layer
Xá»­ lÃ½ vÃ  chuáº©n bá»‹ dá»¯ liá»‡u cho training

#### Chá»©c nÄƒng chÃ­nh:

##### ğŸ”¹ Data Augmentation
```python
augment_image(img)
# 1. Random Hue Shift (80% probability)
#    - Chuyá»ƒn RGB â†’ HSV
#    - Dá»‹ch chuyá»ƒn Hue channel
#    - Chuyá»ƒn láº¡i RGB
# 2. Random Rotation (0Â°, 90Â°, 180Â°, 270Â°)
# 3. Random Horizontal Flip (50% probability)
# 4. Random Vertical Flip (50% probability)
```

**Má»¥c Ä‘Ã­ch**: TÄƒng tÃ­nh Ä‘a dáº¡ng cá»§a dá»¯ liá»‡u, giáº£m overfitting

##### ğŸ”¹ Dataset Loading
```python
load_dataset(data_dir, split, max_images, batch_size, norm_stats, image_size)
# 1. Äá»c áº£nh tá»« thÆ° má»¥c pcb/ vÃ  not_pcb/
# 2. Chia dataset: 80% train, 10% val, 10% test
# 3. Resize áº£nh vá» image_size x image_size
# 4. Ãp dá»¥ng augmentation (chá»‰ cho training set)
# 5. Normalize: (X - mean) / std
# 6. One-hot encoding cho labels
# 7. Táº¡o batches
```

**Input/Output**:
- Input: ÄÆ°á»ng dáº«n thÆ° má»¥c, cáº¥u hÃ¬nh
- Output: Batched dataset, normalization stats, file paths

---

### 3. **`train.py`** - Training Orchestration Layer
Äiá»u phá»‘i toÃ n bá»™ quÃ¡ trÃ¬nh training vÃ  evaluation

#### Chá»©c nÄƒng chÃ­nh:

##### ğŸ”¹ Training Loop
```python
train(model, train_dataset, epochs, config, val_dataset, log_file)
# 1. Khá»Ÿi táº¡o early stopping parameters
# 2. VÃ²ng láº·p qua tá»«ng epoch:
#    a. Training phase:
#       - Forward pass vá»›i dropout
#       - TÃ­nh loss
#       - Backward pass
#       - Update parameters
#    b. Validation phase:
#       - Evaluate trÃªn validation set
#       - TÃ­nh metrics (loss, accuracy)
#       - Early stopping check
#    c. Logging:
#       - Ghi metrics vÃ o CSV
#       - In progress ra console
# 3. Tráº£ vá» best model
```

**Hyperparameters**:
```python
config = {
    "learning_rate": 0.001,    # Tá»‘c Ä‘á»™ há»c
    "lambda_reg": 0.01,        # L2 regularization strength
    "keep_prob": 0.7,          # Dropout keep probability
    "patience": 10             # Early stopping patience
}
```

##### ğŸ”¹ Evaluation
```python
evaluation(model, dataset, print_metrics)
# 1. Forward pass trÃªn toÃ n bá»™ dataset (no dropout)
# 2. TÃ­nh confusion matrix: TP, TN, FP, FN
# 3. TÃ­nh metrics:
#    - Accuracy: (TP + TN) / Total
#    - Precision: TP / (TP + FP)
#    - Recall: TP / (TP + FN)
#    - F1 Score: 2 * Precision * Recall / (Precision + Recall)
# 4. Tráº£ vá» dictionary chá»©a táº¥t cáº£ metrics
```

##### ğŸ”¹ Main Function
```python
main()
# 1. Äá»‹nh nghÄ©a kiáº¿n trÃºc model
#    layer_dims = [12288, 512, 256, 128, 64, 2]
#    (Input: 64x64x3 RGB image flattened)
# 2. Load datasets (train, val, test)
# 3. Train model vá»›i early stopping
# 4. LÆ°u best model + normalization stats
# 5. Evaluate trÃªn test set
```

**Input/Output**:
- Input: Configuration, datasets
- Output: Trained model (saved to `model_data.pkl`), training history CSV

---

### 4. **`inference.py`** - Application Layer
Sá»­ dá»¥ng model Ä‘Ã£ train Ä‘á»ƒ dá»± Ä‘oÃ¡n trÃªn áº£nh má»›i

#### Chá»©c nÄƒng chÃ­nh:

##### ğŸ”¹ Load Model
```python
# Load model_data.pkl chá»©a:
# - model: trained parameters
# - norm_stats: (mean, std) for normalization
# - image_size: kÃ­ch thÆ°á»›c áº£nh input
# - test_files: danh sÃ¡ch file test
```

##### ğŸ”¹ Inference
```python
# 1. Chá»n ngáº«u nhiÃªn N áº£nh tá»« test set
# 2. Preprocess:
#    - Resize vá» image_size
#    - Flatten vÃ  normalize vá»›i norm_stats
# 3. Forward pass (no dropout)
# 4. Láº¥y prediction (argmax cá»§a output)
```

##### ğŸ”¹ Visualization
```python
# Hiá»ƒn thá»‹ grid N áº£nh vá»›i:
# - áº¢nh gá»‘c
# - Prediction (PCB / Not PCB)
# - Confidence score
# LÆ°u káº¿t quáº£ vÃ o inference_results.png
```

**Input/Output**:
- Input: Trained model, test images
- Output: Predictions, visualization

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Chuáº©n bá»‹ dá»¯ liá»‡u

Táº¡o cáº¥u trÃºc thÆ° má»¥c:
```
data/
â”œâ”€â”€ pcb/          # áº¢nh PCB
â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”œâ”€â”€ img2.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ not_pcb/      # áº¢nh khÃ´ng pháº£i PCB
    â”œâ”€â”€ img1.jpg
    â”œâ”€â”€ img2.jpg
    â””â”€â”€ ...
```

### 2. Training

```bash
python train.py
```

**Output**:
- `model_data.pkl`: Model Ä‘Ã£ train
- `training_history.csv`: Lá»‹ch sá»­ training

**Console output**:
```
Epoch 1/50 - Train Loss (dropout): 0.5724 - Train Loss (no dropout): 0.0650 - Train Acc: 98.50% - Val Loss: 0.0698 - Val Acc: 98.00%
Epoch 2/50 - Train Loss (dropout): 0.2647 - Train Loss (no dropout): 0.0620 - Train Acc: 98.70% - Val Loss: 0.0699 - Val Acc: 97.90%
...
```

### 3. Inference

```bash
python inference.py
```

**Output**:
- `inference_results.png`: Visualization cá»§a predictions

### 4. Visualize Training History (Optional)

```bash
python plot_history.py
```

**Output**:
- `training_plot.png`: Äá»“ thá»‹ loss vÃ  accuracy qua cÃ¡c epochs

---

## ğŸ¯ Kiáº¿n trÃºc Model

```
Input Layer:    12288 neurons (64x64x3 RGB image flattened)
                  â†“
Hidden Layer 1:  512 neurons (ReLU + Dropout 0.7)
                  â†“
Hidden Layer 2:  256 neurons (ReLU + Dropout 0.7)
                  â†“
Hidden Layer 3:  128 neurons (ReLU + Dropout 0.7)
                  â†“
Hidden Layer 4:   64 neurons (ReLU + Dropout 0.7)
                  â†“
Output Layer:      2 neurons (Softmax)
                  â†“
              [PCB, Not PCB]
```

**Regularization techniques**:
- âœ… Dropout (keep_prob = 0.7)
- âœ… L2 Regularization (lambda = 0.01)
- âœ… Data Augmentation
- âœ… Early Stopping (patience = 10)

---

## ğŸ“Š Hiá»ƒu vá» Metrics

### Train Loss vs Val Loss

**Train Loss (dropout)**: Loss khi training vá»›i dropout
- Cao hÆ¡n vÃ¬ 30% neurons bá»‹ táº¯t ngáº«u nhiÃªn
- Model pháº£i há»c vá»›i Ã­t thÃ´ng tin hÆ¡n

**Train Loss (no dropout)**: Loss thá»±c sá»± cá»§a model trÃªn training set
- TÃ­nh toÃ¡n vá»›i full model (khÃ´ng dropout)
- **So sÃ¡nh metric nÃ y vá»›i Val Loss Ä‘á»ƒ phÃ¡t hiá»‡n overfitting**

**Val Loss**: Loss trÃªn validation set
- Náº¿u Val Loss >> Train Loss (no dropout) â†’ Overfitting
- Náº¿u Val Loss â‰ˆ Train Loss (no dropout) â†’ Good generalization âœ…

---

## ğŸ”§ Dependencies

```
numpy
Pillow (PIL)
pickle (built-in)
```

---

## ğŸ“ Notes

### Táº¡i sao Train Loss cao hÆ¡n Val Loss?

ÄÃ¢y lÃ  hiá»‡n tÆ°á»£ng **bÃ¬nh thÆ°á»ng** khi sá»­ dá»¥ng:
1. **Dropout**: Model yáº¿u hÆ¡n khi training (30% neurons táº¯t)
2. **Data Augmentation**: Training data khÃ³ hÆ¡n (xoay, láº­t, Ä‘á»•i mÃ u)

â†’ Validation sá»­ dá»¥ng full model + áº£nh gá»‘c â†’ Loss tháº¥p hÆ¡n

### Best Practices

1. **LuÃ´n theo dÃµi cáº£ Train Loss (no dropout) vÃ  Val Loss**
2. **Early stopping** dá»±a trÃªn Val Loss, khÃ´ng pháº£i Train Loss
3. **Data augmentation** chá»‰ Ã¡p dá»¥ng cho training set
4. **Normalization stats** pháº£i Ä‘Æ°á»£c tÃ­nh tá»« training set vÃ  Ã¡p dá»¥ng cho táº¥t cáº£ splits

---

## ğŸ“ Learning Resources

File nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng Ä‘á»ƒ há»c deep learning tá»« scratch:
- **model.py**: Hiá»ƒu cÃ¡ch neural network hoáº¡t Ä‘á»™ng á»Ÿ má»©c toÃ¡n há»c
- **data_utils.py**: Hiá»ƒu data preprocessing vÃ  augmentation
- **train.py**: Hiá»ƒu training loop vÃ  optimization
- **inference.py**: Hiá»ƒu cÃ¡ch deploy model

---

## ğŸ“§ Contact

Náº¿u cÃ³ cÃ¢u há»i hoáº·c gáº·p váº¥n Ä‘á», vui lÃ²ng táº¡o issue hoáº·c liÃªn há»‡.